import pickle
from datetime import datetime

def get_filtered_abstracts_by_ids(ids, filtered_abstracts_path):
    """
    Given a list of patent IDs, return a dictionary {id: abstract} 
    by reading from a tab-separated abstracts file.
    """
    abstracts = {}
    ids = set(ids)
    with open(filtered_abstracts_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip().split('\t')
            line = [elem.strip('"') for elem in line]
            if line[0] in ids:
                abstracts[line[0]] = line[1]
    
    return abstracts

def get_topk_labelled_abstracts(k, labelled_ids_path, filtered_abstracts_path):
    """
    Load top-k patent IDs from a pickle and return their abstracts as a dict.
    """
    with open(labelled_ids_path, 'rb') as f:
        labelled_ids = pickle.load(f)
    
    abstracts = get_filtered_abstracts_by_ids(labelled_ids[0:k], filtered_abstracts_path)

    return abstracts


def evaluate_ranking(rankings_path, filtered_citations_path, filing_dates_path, recall_k):
    """
    Evaluates retrieval performance using MAP and Recall@K.

    Args:
        rankings_path (str): Path to a TSV file containing ranked retrieval results. 
                            Each line should be: <query_id>\t<retrieved_id>
        filtered_citations_path (str): Path to the TSV file of ground truth citations generated by data_filtering.py.
        recall_k (int): The cutoff rank K to use for computing Recall@K (e.g., 10, 100, 1000)

    What It Does:
    - Loads the top-ranked retrieval results for each query (a patent ID).
    - Loads ground truth relevant documents (the in-corpus cited patents).
    - For each query, calculates:
        • MAP (Mean Average Precision): Measures how well relevant docs are ranked early.
        • Recall@K: Measures how many relevant docs appear in the top K results.
    - Prints MAP and Recall@K over all queries that had both rankings and ground truth citations.
    """
    from datetime import datetime
    with open(filtered_citations_path, 'r', encoding='utf-8') as citations_file, open(rankings_path, 'r', encoding='utf-8') as rankings_file, open(filing_dates_path, 'rb') as filing_dates_file:
        rankings = {}
        next(citations_file)  # Skip header

        filing_dates = pickle.load(filing_dates_file)


        for line in rankings_file:
            line = line.strip().split('\t')
            if line[0] != line[1]:
                if line[0] not in rankings:
                    rankings[line[0]] = []
                if datetime.strptime(filing_dates[line[1]], "%Y-%m-%d") < datetime.strptime(filing_dates[line[0]], "%Y-%m-%d"):
                    rankings[line[0]].append(line[1])
        
        ground_truths = {}
        for line in citations_file:
            line = line.strip().split('\t')
            if line[0] in rankings:
                if line[0] not in ground_truths:
                    ground_truths[line[0]] = set()
                ground_truths[line[0]].add(line[2])
        
            # ----- Evaluation -----
        average_precisions = []
        recalls = []
        evaluated_queries = 0
        k = recall_k  # You can change this value if needed

    for qid in ground_truths:
        if qid not in rankings:
            continue
        retrieved = rankings[qid]
        relevant = ground_truths[qid]
        if not relevant:
            continue

        # MAP calculation
        hits = 0
        precision_sum = 0.0
        for rank, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                hits += 1
                precision_sum += hits / (rank + 1)
        ap = precision_sum / len(relevant)
        average_precisions.append(ap)

        # Recall@K calculation
        top_k = set(retrieved[:k])
        recall = len(top_k & relevant) / len(relevant)
        recalls.append(recall)

        evaluated_queries += 1

    mean_ap = sum(average_precisions) / evaluated_queries if evaluated_queries else 0.0
    mean_recall = sum(recalls) / evaluated_queries if evaluated_queries else 0.0

    print(f"MAP: {mean_ap:.4f}")
    print(f"Recall@{k}: {mean_recall:.4f} over {evaluated_queries} queries")

def get_primary_claims(ids, api_key):
    """
    Given a list of patent IDs, return {id: first_claim_text}.
    """
    import requests

    url = "https://search.patentsview.org/api/v1/g_claim/"
    headers = {"X-Api-Key": api_key}

    query = {
        "q": {
            "_and": [
                {"patent_id": ids},
                {"claim_sequence": 0}
            ]
        },
        "f": ["patent_id", "claim_sequence", "claim_text"],
        "o": { "size": 1000} 
    }

    resp = requests.post(url, json=query, headers=headers)
    resp.raise_for_status()

    data = resp.json()

    claims_dict = {}
    for item in data.get("g_claims", []):
        text = item.get("claim_text")
        pid = item.get("patent_id")
        if pid and text:
            claims_dict[pid] = text

    return claims_dict