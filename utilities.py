import pickle

def get_filtered_abstracts_by_ids(ids, filtered_abstracts_path):
    """
    Given a list of patent IDs, return a dictionary {id: abstract} 
    by reading from a tab-separated abstracts file.
    """
    abstracts = {}
    ids = set(ids)
    with open(filtered_abstracts_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip().split('\t')
            line = [elem.strip('"') for elem in line]
            if line[0] in ids:
                abstracts[line[0]] = line[1]
    
    return abstracts

def get_topk_labelled_abstracts(k, labelled_ids_path, filtered_abstracts_path):
    """
    Load top-k patent IDs from a pickle and return their abstracts as a dict.
    """
    with open(labelled_ids_path, 'rb') as f:
        labelled_ids = pickle.load(f)
    
    abstracts = get_filtered_abstracts_by_ids(labelled_ids[0:k] ,filtered_abstracts_path)

    return abstracts


def evaluate_ranking(rankings_path, filtered_citations_path, recall_k):
    """
    Evaluates retrieval performance using MAP and Recall@K.

    Args:
        rankings_path (str): Path to a TSV file containing ranked retrieval results. 
                            Each line should be: <query_id>\t<retrieved_id>
        filtered_citations_path (str): Path to the TSV file of ground truth citations generated by data_filtering.py.
        recall_k (int): The cutoff rank K to use for computing Recall@K (e.g., 10, 100, 1000)

    What It Does:
    - Loads the top-ranked retrieval results for each query (a patent ID).
    - Loads ground truth relevant documents (the in-corpus cited patents).
    - For each query, calculates:
        • MAP (Mean Average Precision): Measures how well relevant docs are ranked early.
        • Recall@K: Measures how many relevant docs appear in the top K results.
    - Prints MAP and Recall@K over all queries that had both rankings and ground truth citations.
    """

    with open(filtered_citations_path, 'r', encoding='utf-8') as citations_file, open(rankings_path, 'r', encoding='utf-8') as rankings_file:
        rankings = {}
        next(citations_file)  # Skip header

        for line in rankings_file:
            line = line.strip().split('\t')
            if line[0] != line[1]:
                if line[0] not in rankings:
                    rankings[line[0]] = []
                rankings[line[0]].append(line[1])
        
        ground_truths = {}
        for line in citations_file:
            line = line.strip().split('\t')
            if line[0] in rankings:
                if line[0] not in ground_truths:
                    ground_truths[line[0]] = set()
                ground_truths[line[0]].add(line[2])
        
            # ----- Evaluation -----
        average_precisions = []
        recalls = []
        evaluated_queries = 0
        k = recall_k  # You can change this value if needed

    for qid in ground_truths:
        if qid not in rankings:
            continue
        retrieved = rankings[qid]
        relevant = ground_truths[qid]
        if not relevant:
            continue

        # MAP calculation
        hits = 0
        precision_sum = 0.0
        for rank, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                hits += 1
                precision_sum += hits / (rank + 1)
        ap = precision_sum / len(relevant)
        average_precisions.append(ap)

        # Recall@K calculation
        top_k = set(retrieved[:k])
        recall = len(top_k & relevant) / len(relevant)
        recalls.append(recall)

        evaluated_queries += 1

    mean_ap = sum(average_precisions) / evaluated_queries if evaluated_queries else 0.0
    mean_recall = sum(recalls) / evaluated_queries if evaluated_queries else 0.0

    print(f"MAP: {mean_ap:.4f}")
    print(f"Recall@{k}: {mean_recall:.4f} over {evaluated_queries} queries")